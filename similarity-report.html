<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<title>笔杆检测报告单（全文标明引文）</title>
	<meta name="keywords" content="" />
	<meta name="description" content="" />
          <meta content="0" http-equiv="Expires"/>
          <meta content="no-cache" http-equiv="Pragma"/>
          <meta content="no-cache" http-equiv="Cache-Control"/>
          <meta content="no-cache" http-equiv="Cache"/>
	<link href="css/report.css?v20180524" type="text/css" rel="stylesheet" />
	<script src="js/jquery.tools.pack.js" type="text/javascript"></script>
	<script type="text/javascript">
    function $ShowMore(n) {
        if ($("#simMore_" + n + " a").text() == '收起相似文献') {//收起
            $("#reportTable_" + n + " .trLike").hide();
            for (var i = 0; i < 5; i++) {
                $("#reportTable_" + n + " .trLike:eq("+i+")").show();
            }
            $("#simMore_" + n + " a").html('查看更多相似文献<span class="icons inlineBlock simDown"></span>');
        } else {
            $("#reportTable_" + n + " .trLike").show();
            $("#simMore_" + n + " a").html('收起相似文献<span class="icons inlineBlock simUp"></span>');
        }
}</script>
<style>
    em.similar{color:Red; font-style:normal;}
</style>
</head>
<body>
<div class="report_bg2">
  <div class="report_bg3">
    <div class="report_top">
      
      <h1>笔杆检测报告单<span>（全文标明引文）</span></h1>
    </div>
    <div class="report_Wrap">
      <div class="report_tab" id="report_tab">
       <ul>
                                            <li class="rep_curr"><div><a href="全文标明引文" class="green">全文标明引文</a></div></li>
                                            <li><div><a href="全文对照.html" class="green">全文对照</a></div></li>
        </ul>
        <div class="report_priSav">
          <a href="javascript:window.print();" class="print inlineBlock"><span class="icons inlineBlock"></span>打印</a>
          <a target="_blank" href="https://www.bigan.net/report/explain.html" class="report_explain inlineBlock"><span class="icons inlineBlock"></span>检测说明</a>
        </div>
      </div>
      <div class="report_content">
        <div class="report_main">
          <a id="toTop" title="回到顶部"></a><!-- 回到顶部 -->
          <script>
              $(document).ready(function () {
                  $("#toTop").hide();
                  //检测屏幕高度
                  var height = $(window).height();
                  //scroll() 方法为滚动事件
                  $(window).scroll(function () {
                      if ($(window).scrollTop() > height) {
                          $("#toTop").fadeIn(500);
                      } else {
                          $("#toTop").fadeOut(500);
                      }
                  });
                  $("#toTop").click(function () {
                      $('body,html').animate({ scrollTop: 0 }, 100);
                      return false;
                  });
              });
          </script>
           <div class="report_Mtop"></div>
          <div class="report_Mbot">
            <div class="report_result">
              <div class="report_info">
                <p><span>标题：</span>ICLR26_Rui_Zhu (9)</p>
                <p><span>作者：</span>zhu rui</p>
                <p><span>报告编号：</span>BG202509301657020459</p>
                <p><span>提交时间：</span>2025-09-30 16:57:36</p>
              </div>
              <div class="report_ratio">
                <ul>
                  <li style="display:none;"><span class="icons ratioIcon ratio1"></span>总复制比：<span class="green">7.8%</span></li>
                  <li class="inlineBlock"><span class="icons ratioIcon ratio2"></span>去除引用文献复制比：<span class="green">7.8%</span></li>
                  <li class="inlineBlock"><span class="icons ratioIcon ratio3"></span>去除本人已发表文献复制比：<span class="green">7.8%</span></li>
                  <li class="inlineBlock"><span class="icons ratioIcon ratio4"></span>单篇最大文字复制比：<span class="green">4%</span></li>
                </ul>
              </div>
               <div class="clear"></div>
              <div class="seal">
                <div class="SealArea">
                  <div class="SealBg"><img src="data:image/png;base64,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" /></div>
                  <div class="SealPercent"></div>
                </div>
              </div>
            </div>
            <div class="repeat_words">
              <div class="repWords_box1">
                <div class="repWords_row">
                  <div class="test_range"><a href="https://www.bigan.net/qa/?key=%E6%A3%80%E6%B5%8B%E8%8C%83%E5%9B%B4" target="_blank" class="green"><span class="icons inlineBlock"></span>检测范围</a></div>
                  <span>重复字数:<b class="red">4,622</b></span><span>总字数：<b>59,331</b></span> </div>
                <div class="clear"></div>
              </div>
              </div>
              <div class="clear"></div>
              <div class="similarLiter">

                <h2 style="width:100%;text-align:center;margin-top:15px;">ICLR26_Rui_Zhu (9)_第1部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>Under review as a conference paper at ICLR 2026</p><p>MG2FLOWNET: ACCELERATING HIGH-REWARD</p><p>SAMPLE GENERATION VIA ENHANCED MCTS</p><p>AND GREEDINESS CONTROL</p><p>Anonymous authors</p><p>Paper under double-blind review</p><p>ABSTRACT</p><p>Generative Flow Networks (GFlowNets) have emerged as a powerful tool for gen-erating diverse and high-reward structured objects by learning to sample from adistribution proportional to a given reward function. Unlike conventional rein-forcement learning (RL) approaches that prioritize optimization of a single tra-jectory, GFlowNets seek to balance diversity and reward by modeling the entiretrajectory distribution. This capability makes them especially suitable for do-mains such as molecular design and combinatorial optimization. However, ex-isting GFlowNets sampling strategies tend to overexplore and struggle to con-sistently generate high-reward samples, particularly in large search spaces withsparse high-reward regions. Therefore, improving the probability of generatinghigh-reward samples without sacrificing diversity remains a key challenge underthis premise. In this work, we integrate an enhanced Monte Carlo Tree Search(MCTS) into the GFlowNets sampling process, using MCTS-based policy evalua-tion to guide the generation toward high-reward trajectories and Polynomial UpperConfidence Trees (PUCT) to balance exploration and exploitation adaptively, andwe introduce a controllable mechanism to regulate the degree of greediness. Ourmethod enhances exploitation without sacrificing diversity by dynamically bal-ancing exploration and reward-driven guidance. The experimental results showthat our method can not only accelerate the speed of discovering high-rewardregions but also continuously generate high-reward samples, while preservingthe diversity of the generative distribution. All implementations are available athttps://anonymous.4open.science/r/MG2FlowNet-68B2/.</p><p>1 INTRODUCTION</p><p><em class='similar'>Generative Flow Networks </em><em class='similar'>(GFlowNets)</em><em class='similar'>(Bengio et al.</em><em class='similar'>,2021;</em><em class='similar'> Jain et al.</em>,2022; Gao et al.,2022;<em class='similar'>Bengio et al.</em><em class='similar'>,2023;</em><em class='similar'> Zhang et al.</em>,2025) have recently emerged as a powerful tool for generatingdiverse high-quality candidates by learning to sample from a reward-proportional distribution. Thisproperty makes GFlowNets particularly attractive for a wide range of structured generation tasks.For example, Jain et al.(2022) integrates GFlowNets into an active learning pipeline for biologicalsequence design. In the domain of Bayesian structure learning, Deleu et al.(2022) and Nishikawa-Toomey et al.(2022) employ GFlowNets to model posterior distributions over discrete composi-tional structures such as Bayesian networks. Liu et al.(2023a) utilizes GFlowNets for samplingmodular subnetworks, improving model generalization under distributional shifts.</p><p>Despite these advantages, vanilla GFlowNets often struggle to efficiently discover high reward sam-ples in complex environments. While their inherent exploratory nature enhances diversity, it maylead to excessive coverage of low-reward regions, particularly during early training when the sam-pling policy lacks guidance and relies on self-collected experience. This results in slow convergenceand suboptimal performance in sparse reward scenarios. The fundamental challenge lies in balanc-ing broad exploration with efficient high-reward discovery, highlighting the need for directed ex-ploration strategies that maintain diversity while effectively guiding the model toward high-rewardareas. To address these issues, several recent efforts have incorporated reinforcement learning tech-niques into the GFlowNets framework. Notably, QGFN (Lau et al.,2024) introduces action value(i.e., Q-value) to enhance backward policy estimation, while another approach applies MCTS and</p><p>Under review as a conference paper at ICLR 2026</p><p>maximum entropy regularization (Morozov et al.,2024) to enhance planning capabilities. However,these strategies often rely on noisy or inaccurate value estimates in the early training stages and maystill fail to effectively guide the model toward high-reward regions. Moreover, they typically lack afine-grained mechanism for dynamically adjusting the trade-off between exploration and exploita-tion throughout training. This raises a key question: How can we enhance the model’s ability toexplore high-reward regions early in training, while adaptively using historical experience in laterstages to maintain high-reward sampling?</p><p>s1</p><p>s2</p><p>sn</p><p>Explored regions Unexplored regions</p><p>x0</p><p>R ( x0)=5</p><p>favor</p><p>favor</p><p>disfavor</p><p>s0</p><p>s : intermediate state x : terminal state</p><p>R (x): reward : edge in the DAG</p><p>x1 xi x5</p><p>R ( xi )=1</p><p>x</p><p>R ( x )=8</p><p>initial</p><p>state</p><p>Figure 1: Strategy of MG2FlowNet.MG2FlowNet prioritizes high reward states inexplored regions while still allocating effortto unexplored areas, ensuring that potentialhigh reward states are not overlooked.</p><p>Monte Carlo Tree Search (MCTS) is a best-firstsearch algorithm that has demonstrated strong per-formance in sequential decision-making tasks suchas AlphaGo Zero (Silver et al.,2016;2018). It of-fers an effective way to explore large search spaces.We build on this idea by proposing a frameworkthat integrates Polynomial Upper Confidence Trees(PUCT)-guided MCTS (Coulom,2006; Kocsis &amp;Szepesvári,2006) with a tunable α-greedy sam-pling strategy. As shown in Figure 1, the frameworkdirects GFlowNets toward promising regions of thestate space: in explored areas, MG2FlowNet fa-vors actions leading to high-reward states, while inunexplored areas it still allocates probability massto encourage the discovery of potentially valuablestates. The α-greedy mechanism combines the Q-values estimated from MCTS rollouts with the for-ward policy of GFlowNets, allowing adaptive con-trol over the degree of greediness. This design im-proves the efficiency of reaching high-reward sam-ples while preserving the diversity of exploration.</p><p>Our main contributions are as follows:</p><p>❶ We present a novel integration of enhanced MCTS and Greediness control with GFlowNets</p><p>(termed MG2FlowNet), which demonstrates significant improvements in both sample efficiencyand consistent generation of high reward samples, especially in large, sparse reward domainssuch as molecule design.</p><p>❷ We achieve a better balance between exploration and exploitation. By introducing the PUCT</p><p>method in the selection phase of MCTS, we enable the model to adaptively adjust the intensityof exploration and exploitation.</p><p>❸ We implement a controllable soft greedy strategy. We consider both the Q-value of individual</p><p>nodes and the flow distribution of the flow network, and use the distribution of Q-values as thegreedy term, which enables us to achieve relatively good results even in the early training stagewhen Q-values are unstable.</p><p>❹ We empirically validate MG2FlowNet on several tasks, demonstrating improved sample effi-ciency and high-reward discovery while maintaining the diversity of generated solutions.</p><p>2 RELATED WORK</p><p>GFlowNets. Generative Flow Networks (GFlowNets) were first proposed by Bengio et al.(2021)as a framework for sampling compositional objects with probabilities proportional to their rewards,providing a scalable alternative to classical methods in multimodal or sparse reward settings. Thisformulation enables diverse and efficient exploration, which has proven useful in applications suchas <em class='similar'>biological sequence design </em><em class='similar'>(Jain et al.</em><em class='similar'>,2022)</em><em class='similar'> and Bayesian structure learning </em><em class='similar'>(Deleu et al.</em><em class='similar'>,2022).</em>Theoretical advances further connected GFlowNets to variational inference (Zimmermann et al.,2022). Despite these advances, classical GFlowNets are often prone to inefficient exploration, slow-ing convergence, and reducing the quality of high-reward samples. Our work addresses this draw-back by introducing a mechanism that better balances exploration and exploitation.</p><p>Improving GFlowNets Sampling. MCTS has demonstrated strong performance in sequential de-cision making, most notably in AlphaGo and AlphaZero (Silver et al.,2016;2018). A key refinementis the PUCT algorithm (Coulom,2006; Kocsis &amp; Szepesvári,2006), which integrates visit counts</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">ICLR26_Rui_Zhu (9)_第2部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>Under review as a conference paper at ICLR 2026</p><p>into the selection rule to balance exploration and exploitation. Inspired by these ideas, our work in-corporates PUCT-guided MCTS and controllable greedy strategies into the GFlowNets framework,enabling more efficient trajectory generation while preserving theoretical guarantees.</p><p>3 PROBLEM FORMULATION</p><p>We generate a candidate object from the initial state s0, making a sequence of actions to finallytransfer the state to the terminal state x with probability proportional to a reward function R(x):x→ R+. The state transformation process can be illustrated as a directed acyclic graph (DAG). Wedenote this sequence as a trajectory τ=(s0→...→ x), the set of complete trajectories as T , theset of states as S, the set of terminal states as X , the action as (s → s′), and the set of actions asA ={(s → s′)|s, s′∈ S}. We say s is a parent of s′, and s′ is a child of s. We denote the C(s)as the set of children of s, the P (s) as the set of parents of s. The set of available actions from s isdenoted as A(s), and thus A(x)=∅ for any terminal state x. For any state s, define the state flow</p><p>F (s)=</p><p>∑</p><p>s∈τ F (τ), and for any edge s→ s′, the edge flow F (s→ s′)=</p><p>∑</p><p>τ=(...→s→s′→...) F (τ).</p><p>We denote the outflow of initial state s0 as Z = F (s0)=</p><p>∑</p><p>τ∈T F (τ). The forward and backward</p><p>probabilities are denoted as PF and PB :</p><p>PF (s</p><p><em class='similar'>′| s)= F </em><em class='similar'>(s→ s′)</em><em class='similar'>/F (</em><em class='similar'>s),</em><em class='similar'> PB(</em><em class='similar'>s | s′)</em><em class='similar'>= F (</em><em class='similar'>s→ s′)</em><em class='similar'>/F (</em><em class='similar'>s′).(1)</em>We have the trajectory balance <em class='similar'>(TB)</em> constraint <em class='similar'>(Malkin et al.</em><em class='similar'>,2022)</em> for any complete trajectory</p><p>τ=(s0→...→ sn):</p><p>Z</p><p>n∏</p><p>t=1</p><p>PF (st|st−1)= F (x)</p><p>n∏</p><p>t=1</p><p>PB(st−1|st),(2)</p><p>And the trajectory loss is defined as:</p><p>LTB(τ)=</p><p>(</p><p>log</p><p>Zθ</p><p>∏n</p><p>t=1 PF (st | st−1;θ)</p><p>R(x)</p><p>∏n</p><p>t=1 PB (st−1| st;θ)</p><p>)2</p><p>.(3)</p><p>Our MCTS algorithm constructs an initially empty directed acyclic graph (DAG) and expands itincrementally. We denote the resulting MCTS-DAG by Gm =(Vm,Am) and the GFlowNets sam-pling DAG by G =(V,A), where Vm and V are the sets of nodes, andAm ⊆ Vm×Vm,A ⊆ V×Vare the sets of directed edges (actions). In Gm, individual nodes are denoted by n. To formalize theMCTS iteration process, we denote the expected value of taking action a at node n by Q(n, a), andthe number of times action a has been executed at n by N(n, a). Let T ⊆ Vm be the set of terminalnodes, with each terminal node denoted by nT , and let F ⊆ Vm be the set of fully expanded nodes(i.e.,<em class='similar'> nodes with children)</em><em class='similar'>. Nodes without children are referred to as leaves.</em> The detailed notationsare provided in Table 3.</p><p>4 METHODOLOGY</p><p>Framework. To overcome the limitation that GFlowNets struggle to sample high-reward regionsconsistently, we incorporate modified MCTS for its planning capability and introduce a parameter α</p><p>to link it with the flow network, thereby controlling the level of greediness. As illustrated in Figure 2,we start from the state s0, which has several available actions {a0, a1,..., an}. The objective is tochoose the action most likely to guide the search toward high-reward regions. Before making thischoice, we perform I rounds of MCTS on n0(corresponding to the s0 in G) in Gm. Each roundconsists of four phases: selection, expansion, simulation, and backpropagation. An iteration usesthe current node history to identify a promising path, simulates it to a terminal state nT , records thereward R(nT ), and then backpropagates this reward to update the statistics Q(n, a) and N(n, a) ofall nodes along the path. After I iterations, actions from s0 yield distinct {Q(s0, ai)| ai ∈ A(s0)}.We then apply a mixed strategy, controlled by α, that combines Q(n, a) with the prior PF to select anaction and move from s0 to s1. This procedure repeats until a terminal state x is reached, producinga trajectory τ=(s0→ s1→→ x). We next detail the four stages and explain how ourframework implements controllable greedy sampling.</p><p>4.1 PUCT GUIDED SELECTION</p><p>The selection phase aims to efficiently reach promising leaf nodes through a balance of explorationand exploitation. During the selection phase, we will meet these situations:(1) The current nodehas no child nodes and is called a leaf node. If it is a terminal node (corresponding to a terminal</p><p>Under review as a conference paper at ICLR 2026</p><p>Selection Expansion Simulation Backpropagation</p><p>Repeat I times</p><p>Selection policy</p><p>based on PUCT</p><p>Terminal</p><p>states</p><p>Expanded</p><p>states</p><p>Expanded</p><p>states</p><p>Updating Q and</p><p>N with R(x)</p><p>Q(s,a)-&gt;Q&#39;(s,a)</p><p>N(s,a)-&gt;N(s,a)+1</p><p>Selection policy</p><p>based on PUCT</p><p>Simulation</p><p>policy Backpropagation</p><p>Sampling based</p><p>on Q and PF</p><p>Case: Current MCTS iteration: Round 4</p><p>R(x)</p><p>Selecting</p><p>x based on PF</p><p>x</p><p>x</p><p>xx</p><p>x</p><p>R(x)</p><p>R(x)</p><p>R(x)</p><p>R(x)</p><p>x</p><p>initial state</p><p>Figure 2: Illustration of framework. The left panel shows trajectory sampling in GFlowNets,where each action is chosen based on the updated Q(s, a) and PF after I rounds of MCTS iterations.The right panel illustrates the MCTS procedure, including selection, expansion, simulation, andbackpropagation, with the fourth iteration shown as an example for clarity.</p><p>state x ∈ X ), then we return the reward R(x) and start the backpropagation stage, and we use −2 inthe code to indicate this case; else we add all legal child nodes to this node, which is the expansionphase, and we use −1 in the code to indicate this case.(2) The current node has child nodes. Definethe π(n, a) as the selection strategy for taking action a from node n to its child node n′. We definea selected trajectory τ=(n0→...→ ni) from the node n0 corresponding to the current state s0 tothe leaf node ni.</p><p>select(n)=</p><p>{</p><p>(n,−2) if n ∈ T</p><p>(n,−1) if n /∈ F</p><p>select(π(n, a)) if n ∈ F</p><p>(4)</p><p>The selection strategy π(n, a) during the selection phase warrants careful consideration. An overlygreedy approach that relies solely on the Q-value predicted by Gm may lead to local optima, whileexcessive exploration could slow down convergence. Therefore, we employ the PUCT formula todynamically balance exploration and exploitation, using the exploration coefficient cpuct to activelyadjust exploration intensity. This enables rapid acquisition of high-quality samples while maintain-ing diversity, allowing for greedy generation of high-scoring samples without sacrificing variety.</p><p>PUCT(n, a)= Q(n, a)+ cpuct PF</p><p>√∑</p><p>a′ N(n, a′)</p><p>1+N(n, a)</p><p>,(5)</p><p>ṽa = exp(PUCT(n, a)− max</p><p>a′∈A(s)</p><p>PUCT(n, a′)),(6)</p><p>pa = ṽa/(</p><p>∑</p><p>a′∈A(s) ṽa′).(7)</p><p>The visitation count N(n, a) maintains the exploration statistics for action a at state n. The explo-ration bonus term exhibits an inverse relationship with N(n, a), creating an adaptive exploration-exploitation trade-off: When N(n, a) is small (underexplored), the term dominates to encourageexploration. As N(n, a) grows (well explored), the term decays to prioritize exploitation of high-reward actions. This dynamic balance motivates our selection policy π(n, a)∼ Categorical(pa),which follows a categorical distribution over the action space A(n).</p><p>4.2 EXPANDING ALL LEGAL ACTIONS</p><p>The expansion stage is a process of adding child nodes to the leaf node. Consider the followingscenario: if only a single child node is expanded upon encountering a leaf node, each subsequentvisit to this node would require another expansion operation until the node is fully expanded, re-sulting in significant computational overhead. To address this inefficiency, we propose expandingall legal child nodes during the expansion phase. This expansion approach is computationally jus-tified because the exploration term in our PUCT formulation Eq.(5) guarantees that these newlycreated nodes will be properly prioritized based on their low N(n, a), ensuring they will be system-atically explored in future iterations. These expanded nodes’ initial value of Q(n, a) and N(n, a)will be initialized to zero. Because these nodes lack historical statistics due to their initial state,the GFlowNets’ forward probabilities PF naturally dominate the selection process in subsequentiterations. This design principle is explicitly illustrated in Eq.(5).</p><p>Under review as a conference paper at ICLR 2026</p><p>4.3 SIMULATION USING FORWARD PROBABILITY OF GFLOWNETS</p><p><em class='similar'>To better leverage the model’s forward transition probability PF ,</em><em class='similar'> we follow the forward transitionprobability in our simulation stage.</em> Notably, in the selection stage, we already obtain a trajectoryτ=(n0→...→ ni), then we expand the children of ni. After expansion, we select a node ne fromthese expanded children based on the PF inherent to GFlowNets. And then the simulation samplingprocess is performed starting from ne in line with PF until a terminal node nT ∈ T is reached. Inthe simulation stage, we similarly obtain a trajectory τ′=(ne →...→ nT ) from the child nodene to the terminal node nT . This trajectory τ′ is designed to simulate which terminal nodes can bereached by node ne, and during the subsequent backpropagation phase, the reward values from theseterminal nodes are propagated backward along the selection trajectory τ to the root node. Throughthis process, we gain the capacity to anticipate and evaluate future states multiple steps ahead.</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">ICLR26_Rui_Zhu (9)_第3部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>4.4 BACKPROPAGATION ALONG PROMISING PATHS</p><p>The Q(n, a) update during backpropagation is crucial because it directly governs the accuracy ofnode evaluations in Gm. Our method employs weighted incremental updates for value backpropa-</p><p>gation, where each node’s Q(n, a) and N(n, a) are updated as:</p><p>Q(n, a)← Q(n, a)+</p><p>R(nT )−Q(n, a)</p><p>nvisit</p><p>, n ∈τ(n0→...→ ni),(8)</p><p>nvisit ← nvisit +1.(9)The update for Q(n, a) and N(n, a) is as shown above in Eq.(8) and Eq.(9). The update of N(n, a)aims to control the level of exploration. As N(n, a) increases, the exploration term in Eq.(5) de-creases, leading to reduced exploration of that node. At the same time, the update of Q(n, a) ensuresthat nodes with higher reward values are more likely to be selected. These two components are bothessential and work together to strike a balance between exploration and exploitation. There is a chal-lenge in updating the nodes during the backpropagation phase. Because updating different parentsleads to a completely different distribution of the flow network. We address the backpropagationchallenge by updating only the nodes along the trajectory τ selected during the selection phase.Details are provided in Sec. E.</p><p>4.5 GREEDINESS CONTROL</p><p>To combine the PF and the Q(n, a) prediction accuracy for taking action a at node n, we propose aα-greedy strategy in Eq.(11) to dynamically adjust the proportion between the global flow networkand the value distribution of Q-values.</p><p>pi =</p><p>(Qi −Qmin)∑</p><p>k (Qk −Qmin)</p><p>,(10)</p><p>As shown in Eq.(10), instead of adopting a max Q strategy, we choose to use a softmax policybased on Q-values. This decision stems from our goal of encouraging more goal-directed behavioron top of a learned flow model, rather than pursuing greediness for its own sake. Directly using themaximal Q-value may lead to premature convergence and getting stuck in a local optimum. More-over, Q-values are often inaccurate in the early training stages, making the model highly sensitive toestimation errors. To mitigate these risks, we opt for a soft Q-value policy that balances exploitationand exploration more effectively.</p><p>µ∼ Categorical</p><p>((1−α) PF +α p</p><p>∥(1−α) PF +α p∥1</p><p>)</p><p>.(11)</p><p>In summary, we can adaptively balance exploration and exploitation by PUCT during the selectionphase. We can control the level of greediness in the model by tuning the value of α, and consider boththe global flow network and the value distribution, thereby making it adjustable and controllable.The details of our algorithm are as in Sec. C.</p><p>5 EXPERIMENTS</p><p>In this section, we evaluate the performance of MG2FlowNet on two tasks: Hypergrid and MoleculeDesign. These tasks are designed to test the model under different conditions: the former involveslong action trajectories with sparse rewards, while the latter involves short trajectories with a largeaction space, also under sparse rewards. Our evaluation focuses on two central research questions:</p><p>Under review as a conference paper at ICLR 2026</p><p>❶ How effectively does the model achieve early discovery and sustained generation of high-reward candidates?</p><p>❷ How well does the model maintain diversity among generated candidates?</p><p>To provide a comprehensive view of the model’s capabilities with respect to these questions, wereport a set of carefully chosen metrics. In addition, we examine how key parameters of the modelare learned, and we conduct ablation studies on the joint forward probability PF and MCTS-basedplanning in Sec. F. The experimental setup and results for each task are detailed below.</p><p>5.1 HYPERGRID TASK</p><p>Task Description. We begin our evaluation with the Hypergrid environment introduced by Bengioet al.(2021), a canonical testbed for assessing compositional generalization in GFlowNets. Theenvironment consists of a D-dimensional discrete state space structured as a hypercube with edgelength H , yielding HD distinct states. This task challenges agents to develop long-horizon planningcapabilities while learning from extremely sparse reward signals. The agent initiates each episode atthe origin (0,0,,0)∈ ZD and executes actions by incrementing any single coordinate by 1(i.e.,∆xd =1 for dimension d). From any state, the agent may alternatively choose a termination action</p><p>that yields a reward determined by the following function:</p><p>R(x)= R0+R1</p><p>D∏</p><p>d=1</p><p>I</p><p>(∣∣∣∣ xd</p><p>H −1</p><p>−0.5</p><p>∣∣∣∣∈(0.25,0.5]</p><p>)</p><p>+R2</p><p>D∏</p><p>d=1</p><p>I</p><p>(∣∣∣∣ xd</p><p>H −1</p><p>−0.5</p><p>∣∣∣∣∈(0.3,0.4]</p><p>)</p><p>,(12)</p><p>where I denotes the indicator function, and we adopt the standard parameterization: R0=10−5,</p><p>R1=0.5, R2=2, with grid parameters H =8, D =4.</p><p>Metrics. Since the grid environment is relatively simple with only 16 modes, we adopt the fol-lowing two metrics to evaluate the performance of our model:1) Number of modes, which reflects</p><p>the model’s exploration capacity and structural diversity.2) The ℓ1 error Ex∼p</p><p>[∣∣∣p(x)− R(x)</p><p>Z</p><p>∣∣∣],</p><p>where Z =</p><p>∑</p><p>x R(x), measuring how well the learned sampling distribution p(x) matches the targetreward distribution. This ℓ1 error directly assesses whether the GFlowNets achieves its fundamentalobjective of generating samples with probabilities proportional to their rewards.</p><p>Baselines. We compare MG2FlowNet with representative flow-based baselines like TB andMCMC (Malkin et al.,2022; Bengio et al.,2021; Zhang et al.,2022b), as well as several non-flow-based methods, including PPO (Schulman et al.,2017) and RANDOM-TRAJ (which samples ac-tions uniformly at random). All methods are evaluated under the same grid environment and rewardfunction to ensure fairness. The following sections present the results and analysis of MG2FlowNetin comparison to these baselines across multiple evaluation metrics.</p><p>Effectiveness Evaluation. The right of Figure 3 shows that the number of modes discoveredis a key indicator of how effectively a model identifies high reward candidates. TB recovers 8modes within 20,000 state visits, whereas MG2FlowNet achieves the same within only 10,000 vis-its. Most baselines eventually identify all 16 modes after 40,000 visits. These results indicate thatMG2FlowNet is more effective at locating high reward regions. The underlying reason lies in thedifferent exploration strategies: vanilla GFlowNets emphasize balanced exploration across all candi-date regions, which slows down the process of reaching high-reward areas. In contrast, MG2FlowNetenhances the sampling process through action value prediction and controllable greedy exploration,which systematically biases the trajectories toward promising states. Once certain high-reward re-gions are discovered, the model tends to revisit and exploit those areas more frequently in subsequentiterations, thereby accelerating the discovery of near-optimal solutions and reducing the number ofvisits required to recover all modes.</p><p>Accuracy Evaluation of GFlowNets. The left of Figure 3 reports the ℓ1 error across differ-ent models, which captures the alignment between the generated distribution and the reward-proportional objective of GFlowNets. Vanilla GFlowNets achieve relatively low ℓ1 error by strictlyadhering to the proportionality principle, ensuring that sampling frequencies closely follow rewardmagnitudes. By contrast, MG2FlowNet incorporates the MCTS algorithm to guide the samplingprocess, which introduces a more greedy bias toward high reward regions. As a consequence, oncethe model identifies promising areas, it tends to allocate a greater proportion of its sampling budget</p><p>Under review as a conference paper at ICLR 2026</p><p>10000300005000080000100000</p><p>States visited</p><p>0.0</p><p>0.1</p><p>0.2</p><p>0.3</p><p>0.4</p><p>0.5</p><p>D</p><p>is</p><p>tr</p><p>ib</p><p>ut</p><p>io</p><p>n</p><p>er</p><p>ro</p><p>r</p><p>TB</p><p>MG2FlowNet</p><p>MCMC</p><p>PPO</p><p>RANDOM-TRAJ</p><p>10000200003000040000</p><p>States visited</p><p>N</p><p>um</p><p>be</p><p>r</p><p>of</p><p>m</p><p>od</p><p>es</p><p>TB</p><p>MG2FlowNet</p><p>MCMC</p><p>PPO</p><p>RANDOM-TRAJ</p><p>Figure 3: High Reward Mode Discovery and Distribution Matching Error on Hypergrid. Left:Comparison of the number of high-reward region modes that different models can find with the samenumber of visits. Right: Comparison of ℓ1 loss across models, measuring deviation between learnedsampling distribution and target reward distribution.</p><p>to those regions in later training stages. This intentional departure from exact proportionality leadsto slightly larger ℓ1 error values, but remains consistent with the design goal: prioritizing the rapididentification of promising regions and the generation of high reward candidates. In practice, thisresults in a sampling distribution that, while not perfectly reward proportional, is better suited forproducing near-optimal solutions within fewer training rounds.</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">ICLR26_Rui_Zhu (9)_第4部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>5.2 MOLECULE DESIGN TASK</p><p>Task Description. Recent advances in artificial intelligence have revolutionized computationalchemistry, particularly in molecular property prediction and design (Du et al.,2024; Li et al.,2024;Zhang et al.,2023b). Molecular design presents an ideal application scenario for GFlowNets, as itrequires simultaneous optimization of two critical objectives:1) quality (achieving target chemicalproperties) and 2) diversity (generating structurally distinct candidates). This dual requirement stemsfrom practical drug discovery needs, where viable candidates must not only exhibit strong bindingaffinities but also possess synthesizable structures. We focus on the specific challenge of designingmolecules with maximal binding energy to a target protein. To this end, we formally describe theaction space for molecular generation: building on junction tree-based molecular generation and</p><p>following Bengio et al.(2021), we define:</p><p>A(s)={(v, b)| v ∈ V(s), b ∈ B},(13)where v denotes the choice of target atom, b denotes the choice of building block. where V(s)denotes attachable atoms in state s and B is our building block vocabulary (|B|=105). Givena molecule, a building block can be added to the molecule at different positions. The combinato-rial action space poses significant exploration challenges while enabling the generation of diversemolecular scaffolds.</p><p>Metrics.1) Number of modes, which reflects the model’s exploration capacity and structuraldiversity.2) Average top 100, Among all generated candidate molecules, we report how manymolecular states are visited when the top-100 average reward exceeds 7.0,7.5, and 8.0. Fewervisited states to reach the corresponding average reward indicate faster discovery of high rewardregions.3) Tanimoto similarity, in molecular generation tasks, if all high-reward molecules pro-duced are structurally almost identical, then even high reward values would indicate that the modelsuffers from mode collapse. To address this, we additionally adopt the Tanimoto similarity metric,which measures the structural differences among generated molecules and further reflects whetherthe model can maintain diversity while consistently generating high-reward molecules.</p><p>Baselines. we compare MG2FlowNet with four popular flow-based baselines,<em class='similar'> TB (</em><em class='similar'>Malkin et al.</em><em class='similar'>,2022),</em><em class='similar'> SubTB </em><em class='similar'>(Madan et al.</em><em class='similar'>,2023),</em><em class='similar'> DB (</em><em class='similar'>Bengio et al.</em>,2021) and QGFN (Lau et al.,2024). Here,we adopt the same training parameters as the vanilla GFlowNets.</p><p>Under review as a conference paper at ICLR 2026</p><p>0200040006000800010000</p><p>Round</p><p>N</p><p>um</p><p>be</p><p>r</p><p>of</p><p>m</p><p>od</p><p>es</p><p>(a) Modes with reward &gt;7.5</p><p>TB</p><p>SubTB</p><p>DB</p><p>QGFN</p><p>MG2FlowNet</p><p>0200040006000800010000</p><p>Round</p><p>N</p><p>um</p><p>be</p><p>r</p><p>of</p><p>m</p><p>od</p><p>es</p><p>(b) Modes with reward &gt;8.0</p><p>TB</p><p>SubTB</p><p>DB</p><p>QGFN</p><p>MG2FlowNet</p><p>Figure 4: Number of modes with reward &gt;7.5 and &gt;8.0 in molecule design task. Left: Com-parison of different models in terms of the number of modes with reward greater than 7.5. Right:Comparison of different models in terms of the number of modes with reward greater than 8.0.</p><p>Effectiveness Evaluation. Figure 4 reports the discovery of high reward samples. SubTB and DBshow significantly inferior performance, as no high reward samples (with reward &gt;7.5 or &gt;8.0)are discovered even after 10,000 iterations. TB performs slightly better than SubTB and DB, but itstill lags behind MG2FlowNet and QGFN in efficiently identifying high-reward samples. Notably,MG2FlowNet demonstrates superior effectiveness by discovering high-reward samples earlier andmore consistently. In particular, MG2FlowNet surpasses QGFN in locating samples with reward&gt;8.0, successfully achieving this within only 300 iterations. This improvement stems from theexploration term in our Eq.(5) formulation, which plays a crucial role in the early training phase byadaptively balancing exploration and exploitation, unlike QGFN, which solely relies on Q-values.</p><p>Table 1: Number of states visited for top candidates (loweris better). Bold: best; underline: second best.</p><p>States</p><p>visited</p><p>avg top 100 avg top 100 avg top 100</p><p>&gt;7.0&gt;7.5&gt;8.0</p><p>TB 2,8246,42512,816</p><p>QGFN 2,0002,80010,800</p><p>MG2FlowNet 6449645,204</p><p>Table 1 further confirms these observa-tions with the average top 100(avg top100) rewards. SubTB and DB are ex-cluded, as reaching average top 100 re-wards of 7.0,7.5, or 8.0 would require pro-hibitively many state visits. Both QGFNand MG2FlowNet show marked improve-ments, benefiting from action value guidedsampling. However, due to inaccurate Q-values in the early stages, QGFN some-</p><p>times overestimates intermediate reward regions and tends to waste effort on low-potential paths. Incontrast, MG2FlowNet performs consistently well across all thresholds (7.0,7.5, and 8.0), stronglysupporting the effectiveness of integrating MCTS and the α-greedy strategy into the GFlowNetsframework. These mechanisms enable adaptive balancing throughout training, ultimately leading tofaster and more stable discovery of high-reward samples.</p><p>05000100001500020000</p><p>Trajectories</p><p>0.325</p><p>0.350</p><p>0.375</p><p>0.400</p><p>0.425</p><p>0.450</p><p>0.475</p><p>0.500</p><p>0.525</p><p>Ta</p><p>ni</p><p>m</p><p>ot</p><p>o</p><p>si</p><p>m</p><p>ila</p><p>ri</p><p>ty</p><p>TB</p><p>SubTB</p><p>QGFN</p><p>DB</p><p>MG2FlowNet</p><p>Figure 5: The Tanimoto similarity among the top-1000 molecules with the highest rewards generatedby different models.</p><p>Diversity Evaluation. Diversity is as-sessed through the Tanimoto similarityin Figure 5, which reflects the similarityamong generated samples. The resultsshow that MG2FlowNet maintains a lowTanimoto similarity, indicating that evenwhile adopting a greedy sampling strategy,it still preserves diversity to a large extent.This complements the evidence from thenumber of modes: although our modeldiscovers substantially more modes thanbaselines, these are not redundant or trivialrepetitions, as the Tanimoto similarity doesnot increase significantly. Together, thesefindings demonstrate that MG2FlowNet notonly generates more high-reward candidates</p><p>but also achieves this while maintaining diverse coverage of the search space.</p><p>Under review as a conference paper at ICLR 2026</p><p>5.3 ABLATION STUDY OF GREEDINESS COEFFICIENT α</p><p>Table 2: Number of modes (average top &gt;8.0) discov-ered under different α settings. Bold: best in each column.Temp denotes that α is linearly annealed from 0 to 0.2 dur-ing training.</p><p>Model settings 12,00024,00040,000</p><p># of modes (avg top 100&gt;8.0)</p><p>c puct =1,α=0.292175459</p><p>c puct =1,α=0.41746249</p><p>c puct =1, T emp 126103</p><p>c puct =1,α=01461105</p><p>We conduct ablation studies to investigatethe role of the MCTS component in ourframework, and to analyze how controllingthe degree of integration between MCTSand GFlowNets influences model perfor-mance. As illustrated in the Table.2, Weinvestigate the effect of different greedinesscoefficients on model performance. Addi-tionally, we introduce a temperature coeffi-cient to realize a dynamic strategy that pro-motes stronger exploration during the early</p><p>phase of training and gradually shifts toward greedier exploitation in later stages.</p><p>Greedy sampling (α=0.4). We initially hypothesized that a higher value of the α-greedy param-eter, which corresponds to a more exploitative sampling strategy, would lead to better performancein the early stage of training but worse performance later on. The intuition was that a greedier policywould favor seemingly high-reward samples at the beginning, thus boosting the average top-k scoretemporarily. However, the empirical results contradicted our expectations. We attribute this to ex-cessive greediness, which causes premature convergence to suboptimal regions that initially appearpromising but in fact correspond to low-reward trajectories, ultimately degrading long-term perfor-mance. This observation confirms that setting α too high has a detrimental impact on performance.Temperature controlled strategy (α from 0 to 0.2). The temperature-controlled strategy theoret-ically offers more robust training dynamics. However, the empirical results diverged substantiallyfrom our expectations. We consider that this discrepancy arises from the configuration of the transi-tion steps. Since determining the optimal value for this setting is nontrivial and beyond the primaryscope of this work, we did not further pursue this direction. Importantly, this does not affect the coreperformance of our model, as the analysis was conducted as an auxiliary study. We attribute theobserved performance degradation to the initially very small value of α, which effectively reducesthe model to a standard GFlowNets. Because our PUCT-based selection still incorporates an explo-ration term that dominates in the early stages of training, as discussed in our analysis of cpuct, theresult is additional over-exploration on top of the base GFlowNets behavior, leading to a significantperformance drop. From these observations, we conclude that the temperature-based schedulingof α is undesirable for two reasons: first, it is difficult to precisely control the rate of change; andsecond, the model often exhibits negative performance gains in the early phase. Consequently, weempirically determine that a fixed value of α=0.2 provides the best trade-off.</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">ICLR26_Rui_Zhu (9)_第5部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>Effect of MCTS. When α=0, the model degenerates to a vanilla GFlowNets, yielding fewermodes than α=0.2 or α=0.4, which confirms the utility of the MCTS component in guidingsampling toward high reward regions. Although scheduling α to increase over training rounds leadsto worse performance than α=0, this can be explained by excessive exploration on an unstableflow network in the early stage, causing a significant drop in performance. These results validate theeffectiveness of the MCTS component.</p><p>6 CONCLUSION</p><p>In this paper, we introduce MG2FlowNet, a novel framework that integrates enhanced MCTS withcontrollable greediness into GFlowNets by adapting the selection, simulation, and backpropagationstages to DAG-structured environments. Our method employs a PUCT-based selection policy to-gether with a tunable greediness mechanism to achieve a principled balance between explorationand exploitation. Through extensive experiments, we demonstrate that MG2FlowNet substantiallyimproves both sample efficiency and diversity, particularly in large-scale and sparse-reward set-tings such as molecular generation. Overall, our study highlights the feasibility and effectiveness ofcombining MCTS with GFlowNets, providing insights for developing more powerful reinforcementlearning algorithms integrated with GFlowNets. We also envision extending this approach or othermore effective methods to dynamic environments where the action space and reward function evolveover time, thereby addressing more challenging tasks.</p><p>Under review as a conference paper at ICLR 2026</p><p>ETHICS STATEMENT</p><p>This research relies exclusively on publicly available benchmark environments from Malkin et al.(2022), including the Hypergrid task and standard molecular design datasets, which contain no per-sonally identifiable or sensitive information. No human or animal subjects were involved, and there-fore no ethical approval was required. We acknowledge that generative modeling techniques, suchas MG2FlowNet, could be misapplied in high-stakes domains, including drug discovery or person-alized recommendation systems. However, our contributions are purely methodological, and allexperiments are restricted to controlled and widely accepted benchmarks. To mitigate risks, weemphasize that any downstream applications of this method should be accompanied by domain-specific safeguards, rigorous evaluation, and appropriate human oversight. No conflicts of interestor external influences are associated with this work.</p><p>REPRODUCIBILITY STATEMENT</p><p>We have taken extensive measures to ensure the reproducibility of our results. The main textand appendix provide full details of model architectures, optimization objectives, training hyper-parameters, and evaluation metrics. Additional experimental settings, ablation studies, and en-vironment specifications are documented in the supplementary material. We have released theanonymized source code, configuration files, and preprocessing scripts, which are available athttps://anonymous.4open.science/r/MG2FlowNet-68B2/. With the released re-sources and instructions, independent researchers are able to reproduce all reported results reliably.</p><p class='uncheck'>REFERENCES</p><p class='uncheck'>Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow</p><p class='uncheck'>network based generative models for non-iterative diverse candidate generation. Advances inNeural Information Processing Systems, 34:27381-27394, 2021.</p><p class='uncheck'>Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio.Gflownet foundations. Journal of Machine Learning Research, 24(210):1-55, 2023.</p><p class='uncheck'>Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In Internationalconference on computers and games, pp. 72-83. Springer, 2006.</p><p class='uncheck'>Tristan Deleu, António Góis, Chris Emezue, Mansi Rankawat, Simon Lacoste-Julien, Stefan Bauer,</p><p class='uncheck'>and Yoshua Bengio. Bayesian structure learning with generative flow networks. In Uncertaintyin Artificial Intelligence, pp. 518-528. PMLR, 2022.</p><p class='uncheck'>Wenjie Du, Shuai Zhang, Jun Xia Di Wu, Ziyuan Zhao, Junfeng Fang, and Yang Wang. Mmgnn: A</p><p class='uncheck'>molecular merged graph neural network for explainable solvation free energy prediction. In Pro-ceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, pp. 5808-</p><p class='uncheck'>5816, 2024.</p><p class='uncheck'>Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor Coley. Sample efficiency matters: A</p><p class='uncheck'>benchmark for practical molecular optimization. In S. Koyejo, S. Mohamed, A. Agar-wal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Pro-cessing Systems, volume 35, pp. 21342-21357. Curran Associates, Inc., 2022. URL</p><p>https://proceedings.neurips.cc/paper_files/paper/2022/file/</p><p>8644353f7d307baaf29bc1e56fe8e0ec-Paper-Datasets_and_Benchmarks.pdf.</p><p><em class='similar'>Edward J Hu,</em><em class='similar'> Moksh Jain,</em><em class='similar'> Eric Elmoznino,</em><em class='similar'> Younesse Kaddar,</em><em class='similar'> Guillaume Lajoie,</em><em class='similar'> Yoshua Bengio,</em></p><p><em class='similar'>and Nikolay Malkin.</em><em class='similar'> Amortizing intractable inference in large language models.</em> arXiv preprintarXiv:2310.04363,2023.</p><p><em class='similar'>Moksh Jain,</em><em class='similar'> Emmanuel Bengio,</em><em class='similar'> Alex Hernandez-Garcia,</em><em class='similar'> Jarrid Rector-Brooks,</em><em class='similar'> Bonaventure FPDossou,</em><em class='similar'> Chanakya Ajit Ekbote,</em><em class='similar'> Jie Fu,</em><em class='similar'> Tianyu Zhang,</em><em class='similar'> Michael Kilgour,</em><em class='similar'> Dinghuai Zhang,</em><em class='similar'> et al.</em><em class='similar'>Biological sequence design with gflownets.</em><em class='similar'> In International Conference on Machine Learning,</em><em class='similar'>pp.9786-9801.</em><em class='similar'> PMLR,2022.</em></p><p>Under review as a conference paper at ICLR 2026</p><p>Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European conferenceon machine learning, pp.282-293. Springer,2006.</p><p>Salem Lahlou,<em class='similar'> Tristan Deleu,</em><em class='similar'> Pablo Lemos,</em><em class='similar'> Dinghuai Zhang,</em><em class='similar'> Alexandra Volokhova,</em> Alex</p><p>Hernández-Garcıa,<em class='similar'> Léna Néhale Ezzine,</em><em class='similar'> Yoshua Bengio,</em><em class='similar'> and Nikolay Malkin.</em><em class='similar'> A theory of con-tinuous generative flow networks.</em><em class='similar'> In International Conference on Machine Learning,</em><em class='similar'> pp.18269-</em></p><p>18300. PMLR,2023.</p><p><em class='similar'>Elaine Lau,</em><em class='similar'> Stephen Lu,</em><em class='similar'> Ling Pan,</em><em class='similar'> Doina Precup,</em><em class='similar'> and Emmanuel Bengio.</em><em class='similar'> Qgfn:</em> Controllable</p><p>greediness with action values. Advances in neural information processing systems,37:81645-</p><p>81676,2024.</p><p>Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi,</p><p>Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et al. Learning diverse attacks on large languagemodels for robust red-teaming and safety tuning. arXiv preprint arXiv:2405.18540,2024.</p><p>Jiahe Li, Wenjie Du, and Yang Wang. Molclw: Molecular contrastive learning with learn-</p><p>able weighted substructures. In 2024 IEEE International Conference on Bioinformatics andBiomedicine (BIBM), pp.828-831. IEEE,2024.</p><p><em class='similar'>Dianbo Liu,</em><em class='similar'> Moksh Jain,</em><em class='similar'> Bonaventure FP Dossou,</em><em class='similar'> Qianli Shen,</em><em class='similar'> Salem Lahlou,</em><em class='similar'> Anirudh Goyal,</em></p><p><em class='similar'>Nikolay Malkin,</em><em class='similar'> Chris Chinenye Emezue,</em><em class='similar'> Dinghuai Zhang,</em><em class='similar'> Nadhir Hassen,</em><em class='similar'> et al.</em><em class='similar'> Gflowout:</em><em class='similar'>Dropout with generative flow networks.</em><em class='similar'> In International Conference on Machine Learning,</em><em class='similar'> pp.21715-21729.</em><em class='similar'> PMLR,2023a.</em></p><p><em class='similar'>Shuchang Liu,</em><em class='similar'> Qingpeng Cai,</em><em class='similar'> Zhankui He,</em><em class='similar'> Bowen Sun,</em><em class='similar'> Julian McAuley,</em><em class='similar'> Dong Zheng,</em><em class='similar'> Peng Jiang,</em></p><p><em class='similar'>and Kun Gai.</em><em class='similar'> Generative flow network for listwise recommendation.</em><em class='similar'> In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery and Data Mining,</em><em class='similar'> pp.1524-1534,</em><em class='similar'>2023b.</em></p><p>Ziru Liu, Shuchang Liu, Bin Yang, Zhenghai Xue, Qingpeng Cai, Xiangyu Zhao, Zijian Zhang,Lantao Hu, Han Li, and Peng Jiang. Modeling user retention through generative flow networks.<em class='similar'>In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</em>pp.5497-5508,2024.</p><p><em class='similar'>George Ma,</em><em class='similar'> Emmanuel Bengio,</em><em class='similar'> Yoshua Bengio,</em><em class='similar'> and Dinghuai Zhang.</em><em class='similar'> Baking symmetry intogflownets.</em> arXiv preprint arXiv:2406.05426,2024.</p><p><em class='similar'>Kanika Madan,</em><em class='similar'> Jarrid Rector-Brooks,</em><em class='similar'> Maksym Korablyov,</em><em class='similar'> Emmanuel Bengio,</em><em class='similar'> Moksh Jain,</em> An-</p><p><em class='similar'>drei Cristian Nica,</em><em class='similar'> Tom Bosc,</em><em class='similar'> Yoshua Bengio,</em><em class='similar'> and Nikolay Malkin.</em><em class='similar'> Learning gflownets frompartial episodes for improved convergence and stability.</em><em class='similar'> In International Conference on MachineLearning,</em><em class='similar'> pp.23467-23483.</em> PMLR,2023.</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">ICLR26_Rui_Zhu (9)_第6部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>Nikolay Malkin,<em class='similar'> Moksh Jain,</em><em class='similar'> Emmanuel Bengio,</em><em class='similar'> Chen Sun,</em><em class='similar'> and Yoshua Bengio.</em><em class='similar'> Trajectory balance:</em></p><p><em class='similar'>Improved credit assignment in gflownets.</em><em class='similar'> Advances in Neural Information Processing Systems,</em><em class='similar'>35:5955-5967,</em><em class='similar'>2022.</em></p><p>Nikita Morozov, Daniil Tiapkin, Sergey Samsonov, Alexey Naumov, and Dmitry Vetrov. Improvinggflownets with monte carlo tree search. arXiv preprint arXiv:2406.13655,2024.</p><p>Mizu Nishikawa-Toomey, Tristan Deleu, Jithendaraa Subramanian, Yoshua Bengio, and Laurent</p><p>Charlin.<em class='similar'> Bayesian learning of causal structure and mechanisms with gflownets and variationalbayes.</em><em class='similar'> arXiv preprint arXiv:</em><em class='similar'>2211.02763,</em>2022.</p><p><em class='similar'>Ling Pan,</em><em class='similar'> Dinghuai Zhang,</em><em class='similar'> Aaron Courville,</em><em class='similar'> Longbo Huang,</em><em class='similar'> and Yoshua Bengio.</em><em class='similar'> Generative aug-mented flow networks.</em><em class='similar'> arXiv preprint arXiv:</em><em class='similar'>2210.03308,</em>2022.</p><p><em class='similar'>Ling Pan,</em><em class='similar'> Moksh Jain,</em><em class='similar'> Kanika Madan,</em><em class='similar'> and Yoshua Bengio.</em><em class='similar'> Pre-training and fine-tuning generativeflow networks.</em><em class='similar'> arXiv preprint arXiv:</em><em class='similar'>2310.03419,</em>2023a.</p><p><em class='similar'>Ling Pan,</em><em class='similar'> Nikolay Malkin,</em><em class='similar'> Dinghuai Zhang,</em><em class='similar'> and Yoshua Bengio.</em><em class='similar'> Better training of gflownets withlocal credit and incomplete trajectories.</em><em class='similar'> In International Conference on Machine Learning,</em><em class='similar'> pp.26878-26890.</em> PMLR,2023b.</p><p>Under review as a conference paper at ICLR 2026</p><p><em class='similar'>Ling Pan,</em><em class='similar'> Dinghuai Zhang,</em><em class='similar'> Moksh Jain,</em><em class='similar'> Longbo Huang,</em><em class='similar'> and Yoshua Bengio.</em><em class='similar'> Stochastic generativeflow networks.</em><em class='similar'> In Uncertainty in Artificial Intelligence,</em><em class='similar'> pp.1628-1638.</em> PMLR,2023c.</p><p><em class='similar'>John Schulman,</em><em class='similar'> Filip Wolski,</em><em class='similar'> Prafulla Dhariwal,</em><em class='similar'> Alec Radford,</em><em class='similar'> and Oleg Klimov.</em> Proximal policyoptimization algorithms.<em class='similar'> arXiv preprint arXiv:</em><em class='similar'>1707.06347,</em>2017.</p><p><em class='similar'>David Silver,</em><em class='similar'> Aja Huang,</em><em class='similar'> Chris J Maddison,</em><em class='similar'> Arthur Guez,</em><em class='similar'> Laurent Sifre,</em><em class='similar'> George Van Den Driessche,</em></p><p><em class='similar'>Julian Schrittwieser,</em><em class='similar'> Ioannis Antonoglou,</em><em class='similar'> Veda Panneershelvam,</em><em class='similar'> Marc Lanctot,</em><em class='similar'> et al.</em><em class='similar'> Masteringthe game of go with deep neural networks and tree search.</em><em class='similar'> nature,</em><em class='similar'>529(7587)</em><em class='similar'>:484-489,</em><em class='similar'>2016.</em></p><p><em class='similar'>David Silver,</em><em class='similar'> Thomas Hubert,</em><em class='similar'> Julian Schrittwieser,</em><em class='similar'> Ioannis Antonoglou,</em><em class='similar'> Matthew Lai,</em><em class='similar'> Arthur Guez,</em></p><p><em class='similar'>Marc Lanctot,</em><em class='similar'> Laurent Sifre,</em><em class='similar'> Dharshan Kumaran,</em><em class='similar'> Thore Graepel,</em><em class='similar'> et al.</em><em class='similar'> A general reinforcementlearning algorithm that masters chess,</em><em class='similar'> shogi,</em><em class='similar'> and go through self-play.</em><em class='similar'> Science,</em><em class='similar'>362(6419)</em><em class='similar'>:1140-</em></p><p>1144,2018.</p><p><em class='similar'>Fangxu Yu,</em><em class='similar'> Lai Jiang,</em><em class='similar'> Haoqiang Kang,</em><em class='similar'> Shibo Hao,</em><em class='similar'> and Lianhui Qin.</em><em class='similar'> Flow of reasoning:</em><em class='similar'> Efficienttraining of llm policy with divergent thinking.</em><em class='similar'> arXiv preprint arXiv:</em>2406.05673,1(2):6,2024.</p><p><em class='similar'>Dinghuai Zhang,</em><em class='similar'> Ricky TQ Chen,</em><em class='similar'> Nikolay Malkin,</em><em class='similar'> and Yoshua Bengio.</em><em class='similar'> Unifying generative modelswith gflownets and beyond.</em><em class='similar'> arXiv preprint arXiv:</em><em class='similar'>2209.02606,</em>2022a.</p><p><em class='similar'>Dinghuai Zhang,</em><em class='similar'> Nikolay Malkin,</em><em class='similar'> Zhen Liu,</em><em class='similar'> Alexandra Volokhova,</em><em class='similar'> Aaron Courville,</em><em class='similar'> and Yoshua</em></p><p>Bengio.<em class='similar'> Generative flow networks for discrete probabilistic modeling.</em><em class='similar'> In International Confer-ence on Machine Learning,</em><em class='similar'> pp.26412-26428.</em> PMLR,2022b.</p><p><em class='similar'>Dinghuai Zhang,</em><em class='similar'> Ricky Tian Qi Chen,</em><em class='similar'> Cheng-Hao Liu,</em><em class='similar'> Aaron Courville,</em><em class='similar'> and Yoshua Bengio.</em><em class='similar'> Diffu-sion generative flow samplers:</em><em class='similar'> Improving learning signals through partial trajectory optimization.</em><em class='similar'>arXiv preprint arXiv:</em><em class='similar'>2310.02679,</em><em class='similar'>2023a.</em></p><p>Jiahui Zhang, Wenjie Du, Di Wu, Jiahe Li, Shuai Zhang, and Yang Wang. Improving efficiency in</p><p>rationale discovery for out-of-distribution molecular representations. In 2023 IEEE InternationalConference on Bioinformatics and Biomedicine (BIBM), pp.401-407. IEEE,2023b.</p><p>Yudong Zhang, Xuan Yu, Xu Wang, Zhaoyang Sun, Chen Zhang, Pengkun Wang, and Yang</p><p>Wang. COFlownet: Conservative constraints on flows enable high-quality candidate genera-tion. In The Thirteenth International Conference on Learning Representations,2025. URLhttps://openreview.net/forum?id=tXUkT709OJ.</p><p><em class='similar'>Heiko Zimmermann,</em><em class='similar'> Fredrik Lindsten,</em><em class='similar'> Jan-Willem van de Meent,</em><em class='similar'> and Christian A Naesseth.</em><em class='similar'> Avariational perspective on generative flow networks.</em><em class='similar'> arXiv preprint arXiv:</em><em class='similar'>2210.07992,</em>2022.</p><p>Under review as a conference paper at ICLR 2026</p><p>CONTENTS</p><p>1 Introduction 1</p><p>2 Related Work 2</p><p>3 Problem Formulation 3</p><p>4 Methodology 3</p><p>4.1 PUCT Guided Selection ...............................3</p><p>4.2 Expanding All Legal Actions .............................4</p><p>4.3 Simulation Using Forward Probability of GFlowNets ................5</p><p>4.4 Backpropagation Along Promising Paths .......................5</p><p>4.5 Greediness Control ..................................5</p><p>5 Experiments 5</p><p>5.1 Hypergrid Task ....................................6</p><p>5.2 Molecule Design Task ................................7</p><p>5.3 Ablation Study of Greediness Coefficient α.....................9</p><p>6 Conclusion 9</p><p>A Notation 14</p><p>B Background 14</p><p>B.1 Generative Flow Networks (GFlowNets).......................14</p><p>B.2 Monte Carlo Tree Search ...............................15</p><p>C Detailed Algorithm 15</p><p>D Related Work 15</p><p>D.1 Generative Flow Networks (GFlowNets).......................15</p><p>D.2 Reinforcement Learning and MCTS in GFlowNets .................16</p><p>D.3 Potential Applications of GFlowNets .........................16</p><p>E Backpropagation Challenge Details 17</p><p>F Additional Experimental Results 18</p><p>F.1 Study on Greediness Term ..............................18</p><p>F.2 Study on Exploration Term ..............................18</p><p>F.3 Different Exploration Coefficient cpuct ........................18</p><p>F.4 Comparison of Different Expanding Strategies ....................19</p><p>G Detailed Experimental Setup 19</p><p>Under review as a conference paper at ICLR 2026</p><p>H Proof of Equation (10)20</p><p>I Discussion 20</p><p>I.1 Limitations ......................................20</p><p>I.2 Future Work ......................................20</p><p>J Use of LLMs 21</p><p>A NOTATION</p><p>This section provides a summary of the key notations throughout this paper. The symbols andcorresponding descriptions are listed in Table 3.</p><p>Table 3: Summary of Key Notations</p><p>Symbol Description</p><p>s0, s</p><p>′, s, st States (initial state s0, intermediate state s′, s, st)</p><p>x Terminal state</p><p>S State space</p><p>X Set of terminal states</p><p>A(s) Available action set at state s</p><p>τ=(s0→→ x) A trajectory from the initial state s0 to a terminal state x</p><p>T Set of trajectories</p><p>F (s) Flow of state s (inflow equals outflow)</p><p>F (s→ s′) Flow from state s to s′</p><p>Z Flow of the initial state s0</p><p>PF (s</p><p>′|s) Forward transition probability from s to s′PB(s|s′) Backward transition probability from s′ to s</p><p>LTB(τ) Trajectory Balance loss</p><p>nT Terminal node in Gm</p><p>n, n0 nodes in Gm (intermediate node n, root node n0)Q(n, a) Estimated value of taking action a at node n in Gm</p><p>N(n, a) Visit count of taking action a at node n in Gm</p><p>R(x) or R(nT ) Reward of terminal state or node</p><p>π(n, a) Policy for selecting action a at node n</p><p>PUCT(n, a) PUCT value of taking action a at node n in Gm</p><p>cpuct Exploration coefficient in PUCT</p><p>α Greediness coefficient</p><p>ṽa Unnormalized score of action a before softmax normalization</p><p>pa Probability of selecting action a after softmax normalization</p><p>ℓ1</p><p>ℓ1 error between generated distribution</p><p>and reward-proportional distribution</p><p>V(s) Set of attachable atoms in state s (molecule design)B Building block vocabulary for molecule generation</p><p>G DAG representing the flow networkGm DAG representing the MCTS policy</p><p>B BACKGROUND</p><p>B.1 GENERATIVE FLOW NETWORKS (GFLOWNETS)</p><p>GFlowNets (Bengio et al.,2021) are a class of generative models designed for sampling composi-tional objects x ∈ X through a sequential construction process. The generation process is formalizedas a trajectory τ=(s0,..., x) over a directed acyclic graph (DAG) G =(S,A), where S represents</p><p>Under review as a conference paper at ICLR 2026</p><p>the set of partially constructed states and A ⊂ S × S denotes valid transitions (e.g., adding a frag-ment to a molecule). The DAG is rooted at a unique initial state s0, and terminal states correspondto fully constructed objects.GFlowNets are trained to satisfy flow balance conditions, ensuring thatthe flow F (s) through states is conserved. Terminal states act as sinks, absorbing flow R(s)(a non-negative reward), while intermediate states balance incoming and outgoing flows. This is expressed</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">ICLR26_Rui_Zhu (9)_第7部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>by the balance equation for any partial trajectory (sn,..., sm):</p><p>F (sn)</p><p>m−1∏</p><p>i=n</p><p>PF (si+1|si)= F (sm)</p><p>m−1∏</p><p>i=n</p><p>PB(si|si+1),(14)</p><p><em class='similar'>where PF and PB are the forward and backward policies,</em><em class='similar'> respectively,</em> representing the fraction offlow directed toward children or parents of a state. For terminal states, F (s)= R(s). PF and PB</p><p>are related to the Markovian flow F as follows:</p><p>PF (s</p><p>′| s)= F (s→ s′)</p><p>F (s)</p><p>, PB(s | s′)=</p><p>F (s→ s′)</p><p>F (s′)</p><p>(15)</p><p>B.2 MONTE CARLO TREE SEARCH</p><p><em class='similar'>Monte Carlo Tree Search </em><em class='similar'>(MCTS)(</em><em class='similar'>Coulom,</em><em class='similar'>2006)</em><em class='similar'> is a best-first search algorithm that combines treesearch with Monte Carlo simulation.</em><em class='similar'> The algorithm iteratively builds a search tree through four keyphases:</em> Selection→ Expansion→ Simulation→ Backpropagation.</p><p>• Selection: Traverse the tree from root to leaf using a tree policy (typically Upper Confi-</p><p>dence Bound for Trees, UCT)(Kocsis &amp; Szepesvári,2006):</p><p>a∗= argmax</p><p>a</p><p>(</p><p>Q(s, a)+ c</p><p>√</p><p>lnN(s)</p><p>N(s, a)</p><p>)</p><p>,(16)</p><p>where Q(s, a) is the action value, N(s) and N(s, a) are visit counts, and c is an explorationconstant.</p><p>• Expansion: When reaching an expandable node, create one or more child nodes represent-ing possible state transitions.</p><p>• Simulation: Perform a Monte Carlo rollout from the expanded node using a default policyto estimate the reward.</p><p>• Backpropagation: Update statistics along the traversed path:</p><p>C DETAILED ALGORITHM</p><p>This section describes the detailed algorithmic flow of our framework, as shown in the Algorithm 15.</p><p>D RELATED WORK</p><p>D.1 GENERATIVE FLOW NETWORKS (GFLOWNETS)</p><p>Since their introduction by Bengio et al.(2021), GFlowNets have attracted increasing attention as aframework for sampling compositional objects with probabilities proportional to their rewards. Thisformulation enables efficient exploration in multimodal or sparse reward settings, where traditionalapproaches often struggle. Subsequent research has expanded both the theoretical foundations andmethodological scope of GFlowNets. For instance, Malkin et al.(2022) and Zimmermann et al.(2022) connected GFlowNets to variational inference, showing advantages when leveraging off-policy data. Methodological improvements have focused on more efficient credit assignment (Panet al.,2022;2023b), while others explored multi-objective generation <em class='similar'>(Jain et al.</em><em class='similar'>,2022)</em> and worldmodeling <em class='similar'>(Pan et al.</em>,2023c). Extensions to unsupervised learning <em class='similar'>(Pan et al.</em><em class='similar'>,2023a)</em> and bias reduc-tion via isomorphism tests (Ma et al.,2024) have further broadened their applicability. From a prob-abilistic modeling perspective, Zhang et al.(2022b) proposed joint training of energy-based models</p><p>Under review as a conference paper at ICLR 2026</p><p>Algorithm 1 MCTS Iterations with Greediness Controlled Sampling</p><p>1: Input: Reward function R : X → R&gt;0, batch size M , model PF with parameters θ, root node n0, number</p><p>of MCTS iterations nplayout, PUCT exploration coefficient cpuct, greediness factor α</p><p>2: Output: MCTS sampling policy</p><p>3: Initialize MCTS graph Gm with root node n0 corresponding to state s0</p><p>4: for i =1 to nplayout do</p><p>5: Selection: Traverse tree from n0 to a leaf ni using PUCT; record path τ=(n0→→ ni)</p><p>6: if ni is terminal then</p><p>7: Backpropagate reward R(ni) along the trajectory τ</p><p>8: else</p><p>9: Expansion: Add A(ni)(all available actions of ni) to Gm</p><p>10: Simulation: Choose one child ne from the children generated during the expansion stage, and roll</p><p>out to terminal node nT using PF</p><p>11: Backpropagation: Propagate reward R(nT ) along τ</p><p>12: end if</p><p>13: end for</p><p>14: Sampling Phase: Use α-greedy over Q-values predicted by Gm and PF to generate new samples</p><p>15:µ∼ Categorical</p><p>(</p><p>(1−α) PF+α p</p><p>∥(1−α) PF+α p∥1</p><p>)</p><p>and GFlowNets, and subsequent work connected GFlowNets with diffusion models <em class='similar'>(Zhang et al.</em><em class='similar'>,2022a;</em><em class='similar'> Lahlou et al.</em><em class='similar'>,2023;</em><em class='similar'> Zhang et al.</em>,2023a). Despite these advances, a persistent limitation ofclassical GFlowNets is their tendency toward inefficient exploration, which slows convergence andreduces the quality of high-reward samples. This work directly targets this drawback by proposinga principled mechanism to improve exploration efficiency.</p><p>D.2 REINFORCEMENT LEARNING AND MCTS IN GFLOWNETS</p><p>Beyond standalone developments, recent efforts have explored integrating reinforcement learningtechniques into GFlowNets, such as QGFN (Lau et al.,2024) and MaxEnt RL connections (Mo-rozov et al.,2024). Relatedly, Monte Carlo Tree Search (MCTS) has achieved remarkable successin sequential decision-making, as demonstrated in AlphaGo and AlphaZero (Silver et al.,2016;2018). A central refinement of MCTS is the Polynomial Upper Confidence Trees (PUCT) algo-rithm (Coulom,2006; Kocsis &amp; Szepesvári,2006), which balances exploration and exploitation by</p><p>incorporating visit counts. However, existing strategies, such as the p-greedy rule:</p><p>πtree (| s)=(1− ps) Softmax (Qtree (s,))+ ps U(C(s)),(17)</p><p>which often fall into local optima when high-scoring nodes dominate Q values, and the uniform</p><p>exploration term ignores prior probabilities from GFlowNets. Entropy regularization has been pro-posed as a remedy, but it passively enforces exploration without leveraging historical statistics suchas visit counts. Inspired by these limitations, we incorporate PUCT-guided selection and control-lable greedy strategies into the GFlowNets framework, enabling more efficient trajectory generationwhile preserving theoretical guarantees.</p><p>D.3 POTENTIAL APPLICATIONS OF GFLOWNETS</p><p>GFlowNets have demonstrated significant potential across multiple domains due to their uniqueability to sample diverse solutions while maintaining reward proportionality. Their strong gener-alization capabilities enable effective handling of unseen states, making them particularly suitablefor exploration-intensive tasks. The technology has shown remarkable success in molecular design,where it outperforms traditional reinforcement learning methods in exploring chemical space whilepreserving synthetic feasibility and drug-like properties.</p><p>Additionally, GFlowNets have emerged as a powerful framework for recommendation systems,demonstrating particular effectiveness in addressing the critical diversity-quality trade-off. Recentstudies have successfully applied GFlowNets to enhance listwise recommendations by maintainingrecommendation quality while significantly improving diversity Liu et al.(2023b), as well as opti-mizing user retention through intelligent exploration strategies Liu et al.(2024). Beyond traditionalrecommendation tasks, GFlowNets have shown remarkable adaptability for large language model</p><p>Under review as a conference paper at ICLR 2026</p><p>fine-tuning across various domains. Notable applications include diverse text generation for sen-tence infilling and chain-of-thought reasoning Hu et al.(2023), adversarial prompt generation in redteaming scenarios Lee et al.(2024), and complex puzzle-solving in domains such as BlocksWorldand Game24 Yu et al.(2024). These applications collectively demonstrate GFlowNets’ versatilityin handling both recommendation tasks and language model optimization challenges.</p><p>E BACKPROPAGATION CHALLENGE DETAILS</p><p>There is a challenge in updating the nodes during the backpropagation phase. Because updatingdifferent parents leads to a completely different distribution of the flow network. There are some al-ternative options:1) The reward R(nT ) is uniformly propagated back to each parent node, such thatif there are n parent nodes, each parent node updates its own Q-value with R(nT )/n.2) Distributethe reward R(nT ) to all parent nodes proportionally based on their relative flow magnitudes withinthe flow network. Each parent node updates its own Q-value with ρR(nT ), where ρ represents theproportion of flow from the parent node to a specific child node.3) the reward R(nT ) is only prop-agated back along the trajectory τ=(n0→...→ ni) in the selection phase, described in detail inSec.4.1. Each node included in the trajectory τ updates its own Q-value with R(nT ).</p><p>n0</p><p>nT1 n1 n2 n3 n4 nT2</p><p>nT3R(nT1)=20</p><p>n0</p><p>nT1 n1 n2 n3 n4 nT6</p><p>nT2 nT3 nT4 nT5</p><p>R(nT3)=40</p><p>R(nT2)=20 R(nT1)=20 R(nT6)=20</p><p>R(nT2)=R(nT3)=R(nT4)=R(nT5)=10</p><p>n : intermediate node nT : terminal node</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">ICLR26_Rui_Zhu (9)_第8部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>R(nT): reward : flow with different size</p><p>root node root node</p><p>Shared node policy</p><p>via different action</p><p>Split node policy</p><p>via different action</p><p>Figure 6: Comparison of different representations for reaching the same state via multipleaction sequences. On the left, identical states are represented by a single shared node; On the right,the same state reached through different action sequences is represented by distinct nodes.</p><p>In our work, we adopt the third method for the following reasons. The first two approaches requireupdating all parent nodes and iteratively propagating these updates further up the tree by updatingthe parents of parents and so on. This results in significant computational overhead. Moreover,the second method incurs additional cost by computing the proportion of flow from each parent toits children, which further increases the computational burden. Furthermore, restricting updates tonodes along the selected path τ serves to emphasize the most promising trajectory. In contrast, thefirst two methods would dilute the relative contribution of this most promising path by distributingcredit more broadly, which is undesirable. For different actions that lead to the same node, we havedesigned a global mapping of state nodes. For identical states, only one node is preserved. Thisapproach also aligns with the objective of flow network training. As shown in Figure 6, if we createmultiple nodes for the same terminal state via different action orders, we will distribute the propor-tion of high reward regions among these nodes, which may prevent the MCTS tree from accuratelyreflecting the high reward characteristics of these terminal nodes. Given these considerations, weopt for the third approach in our experimental design.</p><p>Under review as a conference paper at ICLR 2026</p><p>F ADDITIONAL EXPERIMENTAL RESULTS</p><p>F.1 STUDY ON GREEDINESS TERM</p><p>Table 4: The number of states visited for top-performing candidates. Results with and without theexploration term.</p><p>States</p><p>visited</p><p>Average Top-100</p><p>&gt;7&gt;7.5&gt;8</p><p>MG2FlowNet (without Q)152424049204</p><p>MG2FlowNet (with Q)6449645204</p><p>In this section, we use the Molecule Designexperiment as an example to illustrate the sig-nificant role of the greediness term. The per-formance is evaluated based on the averagereward of the top 100 samples, comparinggreedy and non-greedy variants of the pol-icy. As reported in Table 4, we can observethat achieving the same average top-100 re-ward requires visiting a significantly largernumber of states, indicating that the propor-tion of high-reward samples obtained duringsampling is relatively low. This further sub-</p><p>stantiates the critical importance of the greedy term in the algorithm. The greedy term plays a pivotalrole in guiding the model to sample from high-reward regions of the state space. Without this greedycomponent, the model fails to consistently generate high-reward samples, which fundamentally con-tradicts the original design intent of our approach.</p><p>F.2 STUDY ON EXPLORATION TERM</p><p>Table 5: The number of states visited for different modesdiscovered. Comparison of results with and without the ex-ploration term.</p><p>States visited 4816</p><p>MG2FlowNet (cpuct =0)6,41619,21649,616</p><p>MG2FlowNet (cpuct =0.2)4,8169,61620,816</p><p>To illustrate the critical role of theexploration term, we use the Hy-pergrid experiment as an example.As shown in the ablation study re-sults in Table 5, the model performssignificantly worse when the explo-ration term is removed, with perfor-mance reduced to less than half ofthat achieved with the term included.This clearly demonstrates the impor-tance of the exploration component.</p><p>We conclude that the exploration term plays a crucial role, especially in the early stages of training.During this phase, the Q-values are still inaccurate and highly uncertain. Without the explorationterm, the agent tends to exploit unreliable estimates, leading to unstable learning dynamics andultimately degraded performance.</p><p>Table 6: Number of modes discovered under two thresholds. Bold numbers are the highest intheir respective column.</p><p>Configuration average top &gt;7.5 average top &gt;8.0</p><p>12,00024,00040,00012,00024,00040,000</p><p>MG2FlowNet (cpuct =0.5,α=0.2)4029471,879128312704</p><p>MG2FlowNet (cpuct =2,α=0.2)5469231,645189301612</p><p>MG2FlowNet (cpuct =1,α=0.2)5071,0802,8481653841,053</p><p>MG2FlowNet (cpuct =1,α=0.4)743652,10621102783</p><p>MG2FlowNet (cpuct =1, Temp)41701,123143398</p><p>F.3 DIFFERENT EXPLORATION COEFFICIENT cpuct</p><p>As reported in Table 6, empirical results reveal that reducing the exploration coefficient cpuct signif-icantly impairs the model’s ability to consistently generate high reward samples. This observationsupports our initial hypothesis: a smaller cpuct limits the model’s capacity to explore less-visitedregions of the state space, potentially causing it to overlook promising high-reward areas during</p><p>Under review as a conference paper at ICLR 2026</p><p>the early training phase. On the other hand, setting cpuct to a relatively large value leads to strongerearly-stage performance, as it encourages broader exploration and facilitates early discovery of high-reward trajectories. However, as training progresses, such high exploration settings begin to exhibitdiminishing returns and even hinder further progress. While early identification of promising regionsmight be expected to guide subsequent sampling toward them, the problem actually arises from animbalance between exploration and exploitation. When cpuct is set excessively large, the explorationterm in the PUCT (Eq.(5)) selection formula dominates, leading to over-exploration and suboptimalconvergence. Based on extensive empirical evaluations, we find that cpuct =1 provides a favorabletrade-off between exploration and exploitation throughout the training process.</p><p>F.4 COMPARISON OF DIFFERENT EXPANDING STRATEGIES</p><p>In the main text, we mentioned that our expanding strategy is adding all child nodes to the unex-panded node, because the exploration term in our PUCT formulation (Eq.(5)) guarantees that thesenewly created nodes will be properly prioritized based on their low nvisit, ensuring they will besystematically explored in future iterations.</p><p>To better validate the rationality of our design, we conducted a comparative experiment in the Hy-pergrid environment, comparing the approach of expanding all child nodes versus expanding onlyone child node. The goal was to measure the number of states visited required to discover thesame number of modes. If discovering the same modes requires visiting significantly more states,it indicates wasted MCTS iterations and lower state-visit efficiency, making such an approach less</p><p>desirable. The results of this comparative experiment are shown below:</p><p>Table 7: The number of states visited for differ-ent modes discovered. Results of comparing theapproach of expanding all child nodes versus ex-panding only one child node, fewer states visited todiscover the same number of modes, indicate higherexploration efficiency.</p><p>States visited 4816</p><p>MG2FlowNet (all)4,8169,61620,816</p><p>MG2FlowNet (one)9,616134,416/</p><p>Since the strategy of expanding only onechild node requires an impractically largenumber of state visits to discover all 16modes (rendering it meaningless for compar-ison), we omit this result here. However, thestate visit counts required for discovering 4and 8 modes clearly demonstrate the infeasi-bility of single child expansion. Our resultsshow that this approach significantly reducesexploration efficiency.</p><p>We attribute this inefficiency to the funda-mental limitation of single child expansion:Each training iteration predominantly revisitspreviously explored nodes due to constrained</p><p>graph width in MCTS. This severe restriction on new node access dramatically reduces the explo-ration space. Even in our grid experiment, the state visit counts reached alarming magnitudes, letalone in molecular experiments with exponentially larger state spaces where such costs would be-come computationally prohibitive.</p><p>These experimental results conclusively validate our design rationale: expanding all valid childnodes during the expansion phase is essential for achieving optimal state visitation and explorationefficiency.</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">ICLR26_Rui_Zhu (9)_第9部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>G DETAILED EXPERIMENTAL SETUP</p><p>Parameter Setup in Hypergrid Task. For the GFlowNets policy model, we use the same config-</p><p>uration as vanilla GFlowNets, and we sampled trajectories with a batch size of 16, using the Adamoptimizer with all other parameters at their default values. All experiments in this task are performedon a CPU. The horizon and dimension are set to 8 and 4. For the MCTS framework, we set the num-ber of MCTS iterations to 1, the maximum depth of simulation to 20, the exploration coefficient to1, and the greediness factor to 0.2.</p><p>Parameter Setup in Molecule Design Task. For the GFlowNets policy model, we use the datasetand proxy model provided by Bengio et al.(2021); Lau et al.(2024); Malkin et al.(2022). Differentfrom the hypergrid experiment, due to the large state space of this experiment, we set the number of</p><p>Under review as a conference paper at ICLR 2026</p><p>MCTS iterations to 1, the maximum depth of simulation to 8, the exploration coefficient to 1, andthe greediness factor to 0.2.</p><p>H PROOF OF EQUATION (10)</p><p>■ Recall Equation (10):</p><p>pi =</p><p>(Qi −Qmin)∑</p><p>k (Qk −Qmin)</p><p>.</p><p>As shown in Eq.(10), instead of adopting a max-Q strategy, we choose to use a softmax policybased on Q-values. This decision stems from our goal of encouraging more goal-directed behavioron top of a learned flow model, rather than pursuing greediness for its own sake. Directly using themaximal Q-value may lead to premature convergence and get stuck in a local optimum. Moreover,Q-values are often inaccurate in the early training stages, making the model highly sensitive toestimation errors. To mitigate these risks, we opt for a soft value policy Q that more effectivelybalances exploitation and exploration.</p><p>■ Proof. As for node n, we define the Q-values of the child nodes of node n as a set{Q1, Q2,..., Qn}. In order to compute the Q-values distribution of these child nodes, we normalize</p><p>the data of Q-values,</p><p>Q̂i =</p><p>Qi −Qmin</p><p>Qmax −Qmin</p><p>.(18)</p><p>Then, we need to obtain the probabilities of these child nodes. Pi = Q̂i/</p><p>∑</p><p>k Q̂k, so we can obtain</p><p>this result:</p><p>pi =</p><p>(Qi −Qmin)/(Qmax −Qmin)∑</p><p>k(Qk −Qmin/Qmax −Qmin).(19)</p><p>By normalizing both numerator and denominator through division by Qmax − Qmin, the formula</p><p>can be simplified to:</p><p>pi =</p><p>Qi −Qmin∑</p><p>k(Qk −Qmin)</p><p>.(20)</p><p>Therefore, we finally obtain Eq.(10).</p><p>I DISCUSSION</p><p>I.1 LIMITATIONS</p><p>The present study is conducted in controlled environments where both the action space and thereward function remain fixed. This setting is sufficient for validating the core ideas of MG2FlowNetand provides a clear basis for comparison across methods. However, it does not cover scenarioswhere the available actions evolve during training or where the reward distribution changes overtime. These cases are outside the scope of this work and will be investigated in future studies.</p><p>I.2 FUTURE WORK</p><p>Building on the strengths of MG2FlowNet, a natural extension is to adapt the framework to moredynamic and realistic environments. One promising direction is to incorporate mechanisms that canflexibly accommodate evolving action sets, enabling the model to remain effective as the space ofavailable choices expands or shifts. Another direction is to develop adaptive strategies for nonsta-tionary reward distributions, where feedback signals change due to external interventions or shiftingobjectives. Possible solutions include adaptive exploration policies, meta-learning techniques thattransfer knowledge across tasks, or hybrid methods that couple MCTS with fast bandit style esti-mators. Given the scalability and planning capabilities of MG2FlowNet, we believe these extensionswould not only broaden its applicability but also strengthen its role as a general framework forlifelong reinforcement learning and adaptive molecular design.</p><p>Under review as a conference paper at ICLR 2026</p><p>J USE OF LLMS</p><p>In preparing this manuscript, we used a large language model (LLM) as a writing assistant tool,specifically for grammatical refinement, style polishing, and correction of minor typographical er-rors. The LLM did not contribute to the scientific ideas, algorithm design, or experimental setup. Allsubstantive content, reasoning, and conclusions are entirely the product of the authors. We acceptfull responsibility for all content in the paper, including parts refined or corrected by the LLM, andaffirm that no text generated by the LLM constitutes original scientific contributions attributed to it.</p></p>
                </div>
              </div>
             <div class="report_explain2">
              <div class="repExp_left">说明：</div>
              <div class="repExp_rig">
                <p>1.指标是由系统根据《学术论文不端行为的界定标准》自动生成的</p>
                <p>2.本报告单仅对您所选择比对资源范围内检测结果负责</p>
              </div>
            </div>
            <div class="clear"></div>
            <div class="report_footer">
                <div class="assist_tool">
                  <h2>写作辅助工具</h2>
                  <ul>
                    <li>
                      <div class="asst icons asst1"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/recommendtitle/">选题分析</a>
                        <p>帮您选择合适的论文题目</p>
                      </div>
                    </li>
                    <li>
                      <div class="asst icons asst2"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/recommenddata/">资料搜集</a>
                        <p>提供最全最好的参考文章</p>
                      </div>
                    </li>
                    <li>
                      <div class="asst icons asst3"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/recommendoutline/">提纲推荐</a>
                        <p>辅助生成文章大纲</p>
                      </div>
                    </li>
                    <li>
                      <div class="asst icons asst4"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/writing/">在线写作</a>
                        <p>规范写作，提供灵感</p>
                      </div>
                    </li>
                    <li class="bgNo">
                      <div class="asst icons asst5"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/reference/">参考文献</a>
                        <p>规范参考文献，查漏补缺</p>
                      </div>
                    </li>
                  </ul>
                </div>
                <div class="repFot_bot">
                  <div class="reportCopy inlineBlock">版权所有：笔杆 www.bigan.net</div>
                  <div class="shareTo inlineBlock"><span>分享到：</span> <a href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https%3A%2F%2Fwww.bigan.net&title=%E5%92%8C%E6%88%91%E4%B8%80%E8%B5%B7%EF%BC%8C%E6%8B%BF%E8%B5%B7%E7%AC%94%E6%9D%86%EF%BC%8C%E5%81%9A%E4%B8%80%E4%B8%AA%E5%BF%AB%E4%B9%90%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%B8%9D%E3%80%82&summary=%E7%AC%94%E6%9D%86%EF%BC%8C%E4%B8%80%E4%B8%AA%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%86%99%E4%BD%9C%E5%B9%B3%E5%8F%B0%E3%80%82%E7%AC%94%E6%9D%86%E4%B8%BA%E4%BD%A0%E6%89%AB%E6%B8%85%E5%86%99%E4%BD%9C%E6%80%9D%E8%B7%AF%E7%9A%84%E9%98%B4%E9%9C%BE%EF%BC%8C%E6%89%BE%E5%88%B0%E6%89%8D%E6%80%9D%E6%B3%89%E6%B6%8C%EF%BC%8C%E7%81%B5%E6%84%9F%E8%BF%B8%E5%8F%91%E7%9A%84%E5%BF%AB%E6%84%9F%EF%BC%8C%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BF%A1%E6%89%8B%E6%8B%88%E6%9D%A5%E3%80%82www.bigan.net&pics=https%3A%2F%2Fwww.bigan.net%2Flogo_80_80.png" target="_blank" title="QQ空间" class="inlineBlock sitem1 icons pngfix"></a> <a href="#" title="微信" class="inlineBlock sitem2 icons pngfix"></a> <a href="http://service.weibo.com/share/share.php?url=https%3A%2F%2Fwww.bigan.net&title=%E5%92%8C%E6%88%91%E4%B8%80%E8%B5%B7%EF%BC%8C%E6%8B%BF%E8%B5%B7%E7%AC%94%E6%9D%86%EF%BC%8C%E5%81%9A%E4%B8%80%E4%B8%AA%E5%BF%AB%E4%B9%90%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%B8%9D%E3%80%82%EF%BC%88%E7%AC%94%E6%9D%86%EF%BC%8C%E4%B8%80%E4%B8%AA%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%86%99%E4%BD%9C%E5%B9%B3%E5%8F%B0%E3%80%82%E7%AC%94%E6%9D%86%E4%B8%BA%E4%BD%A0%E6%89%AB%E6%B8%85%E5%86%99%E4%BD%9C%E6%80%9D%E8%B7%AF%E7%9A%84%E9%98%B4%E9%9C%BE%EF%BC%8C%E6%89%BE%E5%88%B0%E6%89%8D%E6%80%9D%E6%B3%89%E6%B6%8C%EF%BC%8C%E7%81%B5%E6%84%9F%E8%BF%B8%E5%8F%91%E7%9A%84%E5%BF%AB%E6%84%9F%EF%BC%8C%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BF%A1%E6%89%8B%E6%8B%88%E6%9D%A5%E3%80%82%40%E7%AC%94%E6%9D%86%E7%BD%91%EF%BC%89" target="_blank" title="新浪微博" class="inlineBlock sitem3 icons pngfix"></a> </div>
                </div>
              </div>
           </div>
       </div>
  </div>
</div>
</div>
</body>
</html>